# from langchain_community.vectorstores import Pinecone
from langchain_pinecone import PineconeVectorStore
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.llms.ollama import Ollama
from langchain.chains.retrieval_qa.base import RetrievalQA
from dotenv import load_dotenv
import os
import time
from pinecone import Pinecone as PC

def load_env(dotenv_path=".env"):
    load_dotenv(dotenv_path=dotenv_path)
    base_url = os.getenv("OLLAMA_BASE_URL")
    model_name = os.getenv("OLLAMA_MODEL_NAME")
    if not base_url or not model_name:
        raise ValueError(".env íŒŒì¼ì—ì„œ OLLAMA ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return base_url, model_name

# LLM ì„¤ì • (GCP Ollama ì—°ê²°)
def create_ollama_llm():
    """Create Ollama LLM instance using environment settings."""
    base_url, model_name = load_env()
    system_prompt = load_prompt_template("system_prompt")
    return Ollama(
        model=model_name,
        base_url=base_url,
        temperature=0,
        top_k=20,
        top_p=0.5,
        repeat_penalty=1.2,
        num_ctx=4096,
        system=system_prompt
    )

def load_prompt_template(prompt_file: str) -> str:
    """Load prompt template from file."""
    with open(f"data/prompts/{prompt_file}.txt", "r", encoding="utf-8") as f:
        return f.read()

PINECONE_API_KEY = "pcsk_2taMEH_GfNAiHps89YwWisSUKSyziHmtv18Di3e68w7RJq9j4TCbGKQM9Zch1sVqngHRvX"
INDEX_NAME = "korean-medical-rag"

os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY
os.environ["OLLAMA_BASE_URL"] = "http://34.71.247.126:8888"
def main():
    # ì„ë² ë”© & ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
    start = time.time()
    print("âœ… Pinecone ì—°ê²° ì‹œì‘")
    pc = PC(api_key=PINECONE_API_KEY)
    index = pc.Index(INDEX_NAME)
    print("âœ… Pinecone ì—°ê²° ì™„ë£Œ:", time.time() - start, "ì´ˆ")

    start = time.time()
    print("âœ… Embedding ë¡œë”© ì¤‘...")
    embedding = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    print("âœ… Embedding ì™„ë£Œ:", time.time() - start, "ì´ˆ")
    
    start = time.time()
    print("âœ… Index ë¡œë”© ì¤‘...")
    vectorstore = PineconeVectorStore(index=index, embedding=embedding, text_key="text")
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
    print("âœ… Index retrieval ì™„ë£Œ:", time.time() - start, "ì´ˆ")

    start = time.time()
    print("âœ… Ollama LLM ì—°ê²° ì¤‘...")
    llm = create_ollama_llm()
    print("âœ… LLM ì—°ê²° ì™„ë£Œ:", time.time() - start, "ì´ˆ")

    # QA ì²´ì¸
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True
    )

    # ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
    question = "ë¹„ì†Œì„¸í¬íì•” 3ê¸° ì¹˜ë£Œ ê¶Œê³ ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    print("ğŸ” ì§ˆë¬¸:", question)
    print("ğŸ“¥ ì§ˆë¬¸ ì „ì†¡ ì¤‘...")
    try:
        result = qa.invoke({"query": question})
        print("ğŸ’¬ ë‹µë³€:", result["result"])
        for doc in result["source_documents"]:
            print("ğŸ“„ ì¶œì²˜:", doc.metadata.get("id_"))
            print(doc.metadata.keys())
            print("----------------")
            print(doc.metadata.values())
    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)

if __name__ == "__main__":
    main()