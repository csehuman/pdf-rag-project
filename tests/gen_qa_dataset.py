import os
import json
from docling_.parser import load_all_pdfs
from utils.env_loader import load_env
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate

# 1. Load environment + init LLM
base_url, model_name = load_env()
llm = Ollama(model=model_name, base_url=base_url)

# 2. Prompt template
prompt_template = PromptTemplate.from_template("""
ë‹¤ìŒì€ ì§„ë£Œì§€ì¹¨ ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì§ˆì˜ì‘ë‹µ ìŒì„ {num_questions}ê°œ ìƒì„±í•˜ì„¸ìš”:

ì¡°ê±´:
1. ì§ˆë¬¸ì€ í•´ë‹¹ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ë©°, êµ¬ì²´ì ì¼ ê²ƒ.
2. ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ìµœëŒ€í•œ ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ í•´ì„œ ì‘ì„±í•  ê²ƒ.
3. ë‹µë³€ì€ 1~3ë¬¸ì¥ ë‚´ì™¸ë¡œ êµ¬ì„±í•  ê²ƒ.
4. ì¶œë ¥ì€ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•  ê²ƒ. ì˜ˆ:
[
  {{ "question": "...", "reference_answer": "..." }},
  ...
]

ë¬¸ì„œ ë‚´ìš©:
\"\"\"
{text}
\"\"\"
""")

# 3. Load PDFs
pdf_texts = load_all_pdfs("pdf_data")  # ê²½ë¡œ ìˆ˜ì • í•„ìš”
qa_list = []

# 4. Generate QA for each doc
for idx, text in enumerate(pdf_texts):
    print(f"ğŸ“„ ë¬¸ì„œ {idx + 1}/{len(pdf_texts)} ì²˜ë¦¬ ì¤‘...")

    # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
    text = text[:3000]

    prompt = prompt_template.format(text=text, num_questions=3)
    response = llm.invoke(prompt)

    try:
        qa_pairs = json.loads(response)
        qa_list.extend(qa_pairs)
    except json.JSONDecodeError:
        print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (ë¬¸ì„œ {idx + 1}) ì‘ë‹µ: {response[:200]}...")

# 5. Save to file
with open("tests/pdf_qa_dataset.json", "w", encoding="utf-8") as f:
    json.dump(qa_list, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ì´ {len(qa_list)}ê°œì˜ QA ìŒì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
